---
title: 5 ways to write racist code
date: "2019-03-08T22:40:42.046Z"
---

This is a post for all the resources and links that I have for my [#NICAR19](https://www.ire.org/conferences/nicar-2019/) lighting talk called “5 ways to write racist code”.

[Slides here](http://iamprettydamn.cool/racist-code-slides)

[source code](https://github.com/asg017/racist-code-slides) for slides (warning: it’s disgusting)

## Resources
Links to articles and stories I talked about!

### #5 Use racist data
- [Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3333423)

Extras I didn’t include:

- [Microsoft’s racist twitter bot](https://www.theverge.com/2016/3/25/11306566/microsoft-racist-tay-ai-twitter-chatbot-apology)


### #4 Use non-diverse data

- [Response: Racial and Gender bias in Amazon Rekognition — Commercial AI System for Analyzing Faces. by Joy Buolamwini](https://medium.com/@Joy.Buolamwini/response-racial-and-gender-bias-in-amazon-rekognition-commercial-ai-system-for-analyzing-faces-a289222eeced)

Extras I didn’t include:

- [FaceApp: White is hot](https://techcrunch.com/2017/04/25/faceapp-apologises-for-building-a-racist-ai/)
- [gender data gap](https://amp.theguardian.com/lifeandstyle/2019/feb/23/truth-world-built-for-men-car-crashes?CMP=share_btn_tw&__twitter_impression=true)

### #3 Accidentally

- [Black people in UK 21 times more likely to have university applications investigated, figures show from the Independent](https://www.independent.co.uk/news/education/education-news/uk-black-students-university-applications-investigation-more-likely-ucas-figures-nus-labour-a8314496.html)

extras that I didn’t include:

- [Uber’s accidental redlining](https://twitter.com/rouge8/status/781279125596241920)
- [Same but for seattle](https://medium.com/@SherylCababa/i-combined-the-map-for-seattle-uberhop-with-a-racial-segregation-map-sorry-brown-people-9d756168c8b9)
[Amazon accidental redlining](https://www.bloomberg.com/graphics/2016-amazon-same-day/)

### #2 Use black bloxes

[How to make a racist AI without really trying by Robyn Speer](https://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/)

### #1 Think that code can’t have bias

- [AOC is right about algorithms. from Slate](https://slate.com/news-and-politics/2019/02/aoc-algorithms-racist-bias.html)
- [NYT Opinion piece](https://www.nytimes.com/2019/03/02/opinion/sunday/diet-artificial-intelligence-diabetes.html) (says something like “ML took away the biases of the scientists”)
- [Reddit comment about algorithms “defeating” gerrymandering](https://www.reddit.com/r/politics/comments/5s0mcz/politics_are_a_mess_lets_hand_it_over_to_software/ddbkd71)

## Random questions

> Why did you use spectacle instead of Google slides

It was a series of bad choices.

My original presentation was going to have a cool, built-in “racist AI” that would be trained during the course of the lightning talk. At the end, I’d have a big reveal and say something like “its so easy to write racist code, that this presetnation just made a racist AI!”

I didnt end up doing this because:

1. I found Tensorflowjs to be pretty hard to work with
2. I didn’t have time in the final presentation to do it
3. I didnt want the page to crash
4. I was spending too much time on that and not the presentation

> Example X doesn’t really match with #5/4/3/2/1

Yeah, these examples aren’t too exact - For example, the UK Black applicants story could be about non-diverse data or even racist data - there’s just not enough details. And the Python sentiment analysis tool could fit under “non-diverse” data best.

So these examples aren’t 💯% - but I still think the 5 points stand pretty well on their own feet.

(republished bc I lost the original mdx file)
